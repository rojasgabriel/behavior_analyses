{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7744c8a",
   "metadata": {},
   "source": [
    "## Plot the performance of an animal on one session\n",
    "The goal of this is just to familiarize you with how data are stored, how to plot some quantities and how to plot simple things.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8daff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    # math and array handling \n",
    "import pandas as pd   # some functions to load data and handle tables\n",
    "import pylab as plt   # plotting stuff\n",
    "# from glob import glob # to search files \n",
    "\n",
    "# Note that if you want the plots to be interactive change the %matplotlib option\n",
    "# the option can be \n",
    "#     qt - to use plots in a separate window,\n",
    "#     notebook to use interactive \"widgets\" or\n",
    "#     inline - not interactive\n",
    "%matplotlib inline\n",
    "\n",
    "# load the data from one session\n",
    "filename = '/Users/gabriel/data/GRB004/20230301_161438/chipmunk/GRB004_20230301_161438_chipmunk_DemonstratorAudiTask.h5'\n",
    "session_data = pd.read_hdf(filename) # the task data are in the \"triallog.h5\" files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922206e9",
   "metadata": {},
   "source": [
    "### Display some statistics to understand the data\n",
    "\n",
    " - the total number of trials is the length of the `session_data` variable\n",
    " - the `session_data.response` is `1` for responses to the `leftward`, `0` for `no response` and `-1` for `rightward` licks\n",
    " - the `stim_intensity` is the number of different stimuli conditions present in the dataset.\n",
    " - In the `Droplets` task the stimuli intensity is the **rate to the left minus the rate of events the right**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34530ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrials = len(session_data)\n",
    "print('The subject was doing the task for {0} trials.\\n'.format(ntrials))  # the print function lets you display stuff, it is very useful for debugging \n",
    "\n",
    "ntrials_with_choice = len(session_data[session_data['outcome_record'].isin([0, 1])])\n",
    "print('Out of {0} trials, the subject responded left or right in {1} trials.\\n'.format(ntrials,ntrials_with_choice))\n",
    "\n",
    "# unique_stim_intensities = np.sort(session_data.stim_intensity.unique()) # sort the intensities\n",
    "# print('There are {0} stimuli conditions {1} Hz'.format(len(unique_stim_intensities),unique_stim_intensities))\n",
    "freq_presented = set(len(trial_stim_events) for trial_stim_events in session_data.stimulus_event_timestamps)\n",
    "print(f'There are {len(freq_presented)} stimuli conditions: {freq_presented}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd0764",
   "metadata": {},
   "source": [
    "### Plot the response to each trial versus the stimulus intensity\n",
    "The response to each trial is in the `session_data.response variable`\n",
    "When making plots it gives more control to create a figure using `plt.Figure` and then an `axis` where the plot with actually go into.\n",
    "\n",
    "For visualization purposes we may jitter the points a bit, here we do that by adding uniform noise to the data before plotting. That is only for plotting. We also play with the transparency (alpha value) of the points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb953393",
   "metadata": {},
   "source": [
    "### Plot the psychometric points for these data\n",
    "\n",
    "The psychometric curve for these data can be plot by looking at the responses to one of the sides (left or right). \n",
    "\n",
    "To do this we will plot the probability of responding left (could also be right) for each stimuli intensity.\n",
    "\n",
    "To compute the probability we will use a `for` loop. We will go to each intensity and count the number of trials to the left out of all trials that were done for that intensity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fcc555",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = session_data[session_data.response_side.isin([0,1])] # select only trials where the subject responded\n",
    "sel = sel.reset_index(drop=True)\n",
    "responded_right = np.array(sel.response_side == 1).astype(int) # select the response = 1 (i.e. the left side) and cast to integer datatype (number)   \n",
    "\n",
    "rate_list = np.array([len(timestamps) for timestamps in sel.stimulus_event_timestamps])\n",
    "\n",
    "frequencies = np.array(list(freq_presented))  # the stimulus intensity values\n",
    "p_right = np.zeros_like(frequencies,dtype=float)     # pre-allocate the array (fill with zeros when you know the size)\n",
    "# note that p_left is cast to float so it can take fractional numbers\n",
    "# this is the part where we estimate the probability of left lick\n",
    "for i,frequency in enumerate(frequencies):\n",
    "    p_right[i] = np.sum(responded_right[rate_list == frequency])/np.count_nonzero(rate_list == frequency)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0.2,0.2,0.7,0.7])\n",
    "ax.plot(frequencies,p_right,'ko')\n",
    "ax.set_xticks([4, 8, 12, 16, 20])\n",
    "\n",
    "ax.vlines(12,0,1,color = 'k',lw = 0.3) # plot a vertical line as reference at zero\n",
    "ax.hlines(0.5,np.min(frequencies),np.max(frequencies),color = 'k',lw = 0.3) # plot an horizontal line as reference for chance performance\n",
    "\n",
    "ax.set_ylabel('P$_{right}$',fontsize = 14)  # set the y-axis label with latex nomenclature\n",
    "ax.set_xlabel('Stimulus frequency (event rate)', fontsize = 14); # set the x-axis label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d814cee",
   "metadata": {},
   "source": [
    "### Fit the psychometric curve to trial data\n",
    "\n",
    "It is common to obtain a psychometric function so that we can extract parameters from the fit and compare subjects or conditions.\n",
    "\n",
    "We will fit it with a cumulative gaussian, by minimizing the log likelihood error of all trials. \n",
    "A good course on this is here (where they use the weibull) http://courses.washington.edu/matlab1/Lesson_5.html#6\n",
    "\n",
    "Note that fitting to the average points is not good, because all intensities count the same for the fit but there are some have different number of trials. The log likelihood tries to solve this. \n",
    "\n",
    "This time we will also compute the confidence intervals usign the Wilson binomial confidence interval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b69de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get the average points like above\n",
    "frequencies = np.sort(np.array(list(freq_presented)).astype(float))\n",
    "p_right = np.zeros_like(frequencies,dtype=float) \n",
    "ci_right = np.zeros((len(frequencies),2),dtype=float)\n",
    "\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "for i,frequency in enumerate(frequencies):\n",
    "    cnt = np.sum(responded_right[rate_list == frequency]) # number of times the subject licked left \n",
    "    nobs = np.count_nonzero(rate_list == frequency) # number of observations (ntrials)\n",
    "    p_right[i] = cnt/nobs\n",
    "    ci_right[i] = proportion_confint(cnt,nobs,method='wilson') # 95% confidence interval\n",
    "\n",
    "def cumulative_gaussian(alpha,beta,gamma,lmbda, X):\n",
    "    '''\n",
    "    Evaluate the cumulative gaussian psychometric function.\n",
    "       alpha is the bias (left or right)\n",
    "       beta is the stepness\n",
    "       gamma is the left handside offset\n",
    "       lmbda is the right handside offset\n",
    "      \n",
    "    Adapted from the Palamedes toolbox \n",
    "    Joao Couto - Jan 2022    \n",
    "    '''\n",
    "        \n",
    "    from scipy.special import erfc # import the complementary error function\n",
    "    return  gamma + (1 - gamma - lmbda)*0.5*erfc(-beta*(X-alpha)/np.sqrt(2))+1e-9    \n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def neg_log_likelihood_error(func, parameters, X, Y):\n",
    "    '''\n",
    "    Compute the log likelihood\n",
    "\n",
    "    'func' is the (psychometric) function \n",
    "    'parameters' are the input parameters to 'func'\n",
    "    'Y' is the binary response (correct = 1; incorrect=0)\n",
    "    '''\n",
    "\n",
    "    pX = func(*parameters, X)*0.99 + 0.005  # the predicted performance for X from the PMF\n",
    "    # epsilon to prevent error in log(0)\n",
    "    val = np.nansum(Y*np.log(pX) + (1-Y)*np.log(1-pX))\n",
    "    return -1*val\n",
    "#\n",
    "rate_list = np.array(rate_list)\n",
    "func = lambda pars: neg_log_likelihood_error(cumulative_gaussian, pars, rate_list.astype(float),responded_right.astype(float))\n",
    "# x0 is the initial guess for the fit, it is an important parameter\n",
    "x0 = [0.,0.1,p_right[0],1 - p_right[-1]]\n",
    "\n",
    "bounds = [(frequencies[0],frequencies[-1]),(0.0001,10),(0,0.7),(0,0.7)]\n",
    "\n",
    "res = minimize(func, x0, options = dict(maxiter = 500*len(x0),adaptive=True),\n",
    "               bounds = bounds, method='Nelder-Mead') # method = 'L-BFGS-B',\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0.2,0.2,0.7,0.7])\n",
    "ax.vlines(12,0,1,color = 'k',lw = 0.3) # plot a vertical line as reference at zero\n",
    "ax.hlines(0.5,np.min(frequencies),np.max(frequencies),color = 'k',lw = 0.3) # plot an horizontal line as reference for chance performance\n",
    "ax.set_xticks([4, 8, 12, 16, 20])\n",
    "\n",
    "# plot the fit\n",
    "nx = np.linspace(np.min(frequencies),np.max(frequencies),100)\n",
    "ax.plot(nx,cumulative_gaussian(*res.x,nx),'k')\n",
    "\n",
    "# plot the observed data and confidence intervals\n",
    "for i,e in zip(frequencies,ci_right):  # plot the confidence intervals\n",
    "    ax.plot(i*np.array([1,1]),e,'_-',lw=0.5,color = 'black')\n",
    "    \n",
    "ax.plot(frequencies,p_right,'ko',markerfacecolor = 'lightgray',markersize = 6)\n",
    "\n",
    "ax.set_ylabel('P$_{right}$',fontsize = 18)  # set the y-axis label with latex nomenclature\n",
    "ax.set_xlabel('Stimulus rate', fontsize = 14); # set the x-axis label\n",
    "print('Estimated parameters: alpha {0:2.2f} beta {1:2.2f} gamma {2:2.2f} lambda {3:2.2f}'.format(*res.x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6235c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.sqrt(np.diag(res.hess_inv.todense()))) # The sqrt of the diagonal of the cov mat are the parameter errors? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
